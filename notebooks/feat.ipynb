{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lQ9c0h_USzm8"
      },
      "outputs": [],
      "source": [
        "PATH = \"/content/models\"\n",
        "mapping_pos = [\"Contratado pela Decision\", \"Aprovado\", \"Finalista\", \"Documentação PJ\", \"Encaminhado ao Cliente com Aprovação\"]\n",
        "mapping_neg = [\"Não Aprovado pelo Cliente\", \"Não Aprovado pelo RH\", \"Desistiu\", \"Prospect\", \"Encaminhado ao Requisitante (sem retorno)\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HVzxLhCzWwN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ok] features salvas em '../build': X.npy (53759, 24), y.npy (53759,), groups_job.npy (53759,)\n",
            "[ok] exemplos de features: ['sim_tfidf', 'ingles_ok', 'espanhol_ok', 'senior_ok', 'senior_gap', 'sap_match', 'ctrlm_match', 'sql_match'] ... total=24\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd, numpy as np, re, unicodedata, joblib, json\n",
        "from pathlib import Path\n",
        "from typing import List, Set\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# =========================\n",
        "# Helpers\n",
        "# =========================\n",
        "\n",
        "def norm_txt(t: str) -> str:\n",
        "    t = str(t or \"\")\n",
        "    t = unicodedata.normalize(\"NFKD\", t).encode(\"ascii\",\"ignore\").decode(\"ascii\")\n",
        "    t = re.sub(r\"\\s+\",\" \", t.lower()).strip()\n",
        "    return t\n",
        "\n",
        "def split_areas(s: str) -> List[str]:\n",
        "    s = norm_txt(s)\n",
        "    if not s:\n",
        "        return []\n",
        "    # separadores comuns do dataset\n",
        "    tokens = re.split(r\"[;,/|\\-•–—]+|\\s{2,}\", s)\n",
        "    tokens = [tok.strip() for tok in tokens if tok.strip()]\n",
        "    return tokens\n",
        "\n",
        "def jaccard(a: List[str], b: List[str]) -> float:\n",
        "    sa, sb = set(a), set(b)\n",
        "    if not sa and not sb:\n",
        "        return 0.0\n",
        "    return len(sa & sb) / max(1, len(sa | sb))\n",
        "\n",
        "def map_level(val: str, mapping: dict) -> int:\n",
        "    v = norm_txt(val)\n",
        "    return mapping.get(v, 0)\n",
        "\n",
        "def contains_kw(text: str, kw: str) -> int:\n",
        "    return int(bool(re.search(rf\"\\b{re.escape(kw.lower())}\\b\", norm_txt(text))))\n",
        "\n",
        "def any_kw(text: str, kws: Set[str]) -> int:\n",
        "    text = norm_txt(text)\n",
        "    return int(any(re.search(rf\"\\b{re.escape(k)}\\b\", text) for k in kws))\n",
        "\n",
        "# =========================\n",
        "# Configs simples\n",
        "# =========================\n",
        "\n",
        "# Idiomas (ajuste se quiser granularidade CEFR)\n",
        "MAP_LVL = {\n",
        "    \"nenhum\":0, \"basico\":1, \"básico\":1, \"intermediario\":2, \"intermediário\":2,\n",
        "    \"avancado\":3, \"avançado\":3, \"fluente\":4\n",
        "}\n",
        "\n",
        "# Senioridade (exemplos do dataset)\n",
        "MAP_SENIOR = {\n",
        "    \"estagiario\":1, \"estagiário\":1, \"junior\":2, \"jr\":2, \"analista\":3,\n",
        "    \"pleno\":3, \"senior\":4, \"sênior\":4, \"especialista\":5\n",
        "}\n",
        "\n",
        "# Dicionário inicial de skills (pode crescer depois automaticamente)\n",
        "SEED_SKILLS = {\n",
        "    \"sap\",\"control-m\",\"controlm\",\"sql\",\"pl/sql\",\"oracle\",\"aws\",\"azure\",\"gcp\",\n",
        "    \"linux\",\"windows\",\"vmware\",\"jcl\",\"abap\",\"java\",\"python\",\"etl\",\"bi\",\"powercenter\",\n",
        "    \"connect direct\",\"b2b\",\"devops\",\"git\",\"docker\",\"kubernetes\"\n",
        "}\n",
        "\n",
        "# =========================\n",
        "# Feature builder\n",
        "# =========================\n",
        "\n",
        "def build_features(df: pd.DataFrame, save_dir: str = \"../app/model\"):\n",
        "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # -------- rótulo\n",
        "    POS = {\n",
        "        \"contratado pela decision\",\n",
        "        \"aprovado\",\n",
        "        \"finalista\",\n",
        "        \"documentação pj\",\n",
        "        \"encaminhado ao cliente com aprovação\",\n",
        "        'contratado como hunting',\n",
        "        'documentação clt',\n",
        "        'encaminhar proposta',\n",
        "        'proposta aceita'\n",
        "    }\n",
        "    sit = df[\"situacao_candidado\"].fillna(\"\").map(norm_txt)\n",
        "    y = sit.isin(POS).astype(int).values\n",
        "\n",
        "    # -------- textos base\n",
        "    job_text_raw = (df[\"atividades_vaga\"].fillna(\"\") + \" \" + df[\"competencias_vaga\"].fillna(\"\"))\n",
        "    cv_text_raw  = df[\"cv_texto_pt\"].fillna(\"\")\n",
        "    job_text = job_text_raw.map(norm_txt)\n",
        "    cv_text  = cv_text_raw.map(norm_txt)\n",
        "\n",
        "    # -------- TF-IDF + similaridade\n",
        "    # Usamos um único vocabulário para vaga e CV (mesmo espaço vetorial)\n",
        "    tfidf = TfidfVectorizer(max_features=40000, ngram_range=(1,2), min_df=2)\n",
        "    X_job = tfidf.fit_transform(job_text)\n",
        "    X_cv  = tfidf.transform(cv_text)\n",
        "    sim_tfidf = cosine_similarity(X_job, X_cv).diagonal()\n",
        "\n",
        "    # -------- Idiomas\n",
        "    vi = df.get(\"nivel_ingles_vaga\", \"\").fillna(\"\").map(lambda x: map_level(x, MAP_LVL)).astype(int).values\n",
        "    ci = df.get(\"applicant_nivel_ingles\", \"\").fillna(\"\").map(lambda x: map_level(x, MAP_LVL)).astype(int).values\n",
        "    ingles_ok = (ci >= vi).astype(int)\n",
        "\n",
        "    ve = df.get(\"nivel_espanhol_vaga\", \"\").fillna(\"\").map(lambda x: map_level(x, MAP_LVL)).astype(int).values\n",
        "    ce = df.get(\"applicant_nivel_espanhol\", \"\").fillna(\"\").map(lambda x: map_level(x, MAP_LVL)).astype(int).values\n",
        "    espanhol_ok = (ce >= ve).astype(int)\n",
        "\n",
        "    # -------- Senioridade (diferença vaga - candidato)\n",
        "    vaga_sen = df.get(\"nivel_profissional_vaga\",\"\").fillna(\"\").map(lambda x: map_level(x, MAP_SENIOR)).astype(int).values\n",
        "    cand_sen = df.get(\"applicant_nivel_profissional\",\"\").fillna(\"\").map(lambda x: map_level(x, MAP_SENIOR)).astype(int).values\n",
        "    senior_ok = (cand_sen >= vaga_sen).astype(int)\n",
        "    senior_gap = np.clip(cand_sen - vaga_sen, -3, 3)\n",
        "\n",
        "    # -------- SAP / Control-M / SQL / AWS / Oracle (flags no CV + vaga)\n",
        "    def flag_pair(kw: str):\n",
        "        cv_f  = np.array([contains_kw(t, kw) for t in cv_text_raw], dtype=int)\n",
        "        job_f = np.array([contains_kw(t, kw) for t in job_text_raw], dtype=int)\n",
        "        return cv_f, job_f, (cv_f & job_f)\n",
        "\n",
        "    cv_sap, job_sap, sap_match = flag_pair(\"sap\")\n",
        "    cv_ctrlm, job_ctrlm, ctrlm_match = flag_pair(\"control-m\")\n",
        "    cv_sql, job_sql, sql_match = flag_pair(\"sql\")\n",
        "    cv_aws, job_aws, aws_match = flag_pair(\"aws\")\n",
        "    cv_orc, job_orc, orc_match = flag_pair(\"oracle\")\n",
        "\n",
        "    # -------- Contagem de skills (seed + mining simples a partir das vagas)\n",
        "    # pega top termos de vaga (unigramas) com maior df para virar skill também\n",
        "    vocab = tfidf.get_feature_names_out()\n",
        "    # heurística: unigramas com letras (sem números) e tamanho >=3\n",
        "    mined = {v for v in vocab if (len(v.split())==1 and re.match(r\"^[a-z][a-z0-9\\-_\\.]+$\", v) and len(v)>=3)}\n",
        "    # mantém apenas termos que parecem \"tecnologias\" (simples): presentes em muitos CVs também\n",
        "    # (limita tamanho para não \"inundar\" o feature space)\n",
        "    mined = set(list(mined)[:500])  # limite simples\n",
        "    skills = (SEED_SKILLS | mined)\n",
        "\n",
        "    def count_skills(text: str, skills: Set[str]) -> int:\n",
        "        text = norm_txt(text)\n",
        "        c = 0\n",
        "        for k in skills:\n",
        "            if re.search(rf\"\\b{re.escape(k)}\\b\", text):\n",
        "                c += 1\n",
        "        return c\n",
        "\n",
        "    skills_in_job = np.array([count_skills(t, skills) for t in job_text_raw], dtype=int)\n",
        "    skills_in_cv  = np.array([count_skills(t, skills) for t in cv_text_raw], dtype=int)\n",
        "    # “cobertura”: quantas skills da vaga aparecem no CV (aproximação rápida)\n",
        "    # como proxy: usa interseção por regex simples\n",
        "    def overlap_count(jt: str, ct: str) -> int:\n",
        "        jt = norm_txt(jt); ct = norm_txt(ct)\n",
        "        sj = {k for k in skills if re.search(rf\"\\b{re.escape(k)}\\b\", jt)}\n",
        "        sc = {k for k in skills if re.search(rf\"\\b{re.escape(k)}\\b\", ct)}\n",
        "        return len(sj & sc)\n",
        "    skills_overlap = np.array([overlap_count(j, c) for j, c in zip(job_text_raw, cv_text_raw)], dtype=int)\n",
        "\n",
        "    # -------- Área de atuação (Jaccard)\n",
        "    areas_vaga = [split_areas(s) for s in df.get(\"areas_atuacao_vaga\",\"\").fillna(\"\").tolist()]\n",
        "    areas_cand = [split_areas(s) for s in df.get(\"applicant_area_atuacao\",\"\").fillna(\"\").tolist()]\n",
        "    area_jacc = np.array([jaccard(a, b) for a, b in zip(areas_vaga, areas_cand)], dtype=float)\n",
        "\n",
        "    # -------- Monta matriz X\n",
        "    feats = np.c_[\n",
        "        sim_tfidf,\n",
        "        ingles_ok, espanhol_ok,\n",
        "        senior_ok, senior_gap,\n",
        "        sap_match, ctrlm_match, sql_match, aws_match, orc_match,\n",
        "        cv_sap, cv_ctrlm, cv_sql, cv_aws, cv_orc,\n",
        "        job_sap, job_ctrlm, job_sql, job_aws, job_orc,\n",
        "        skills_in_job, skills_in_cv, skills_overlap,\n",
        "        area_jacc\n",
        "    ].astype(float)\n",
        "\n",
        "    feat_names = [\n",
        "        \"sim_tfidf\",\n",
        "        \"ingles_ok\",\"espanhol_ok\",\n",
        "        \"senior_ok\",\"senior_gap\",\n",
        "        \"sap_match\",\"ctrlm_match\",\"sql_match\",\"aws_match\",\"oracle_match\",\n",
        "        \"cv_sap\",\"cv_ctrlm\",\"cv_sql\",\"cv_aws\",\"cv_oracle\",\n",
        "        \"job_sap\",\"job_ctrlm\",\"job_sql\",\"job_aws\",\"job_oracle\",\n",
        "        \"skills_in_job\",\"skills_in_cv\",\"skills_overlap\",\n",
        "        \"area_jacc\"\n",
        "    ]\n",
        "\n",
        "    # -------- groups por vaga para GroupKFold\n",
        "    groups_job = df[\"id_vaga\"].values\n",
        "\n",
        "    # -------- salva artefatos e dados\n",
        "    np.save(Path(save_dir)/\"X.npy\", feats)\n",
        "    np.save(Path(save_dir)/\"y.npy\", y)\n",
        "    np.save(Path(save_dir)/\"groups_job.npy\", groups_job)\n",
        "\n",
        "    joblib.dump({\n",
        "        \"tfidf\": tfidf,\n",
        "        \"feat_names\": feat_names,\n",
        "        \"map_lvl\": MAP_LVL,\n",
        "        \"map_senior\": MAP_SENIOR,\n",
        "        \"skills_seed\": sorted(SEED_SKILLS),\n",
        "        \"skills_mined_sample\": sorted(list(mined))[:50],  # só para inspecionar\n",
        "    }, Path(save_dir)/\"artifacts.joblib\")\n",
        "\n",
        "    print(f\"[ok] features salvas em '{save_dir}': X.npy {feats.shape}, y.npy {y.shape}, groups_job.npy {groups_job.shape}\")\n",
        "    print(f\"[ok] exemplos de features: {feat_names[:8]} ... total={len(feat_names)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # ajuste o caminho se necessário\n",
        "    df = pd.read_pickle('../app/model/df_final.pkl')\n",
        "    build_features(df, save_dir=\"../app/model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b63m2nlp4hEz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "feat_names: ['sim_tfidf', 'ingles_ok', 'espanhol_ok', 'senior_ok', 'senior_gap', 'sap_match', 'ctrlm_match', 'sql_match', 'aws_match', 'oracle_match', 'cv_sap', 'cv_ctrlm', 'cv_sql', 'cv_aws', 'cv_oracle', 'job_sap', 'job_ctrlm', 'job_sql', 'job_aws', 'job_oracle', 'skills_in_job', 'skills_in_cv', 'skills_overlap', 'area_jacc']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "X = np.load(\"../app/model/X.npy\", allow_pickle=True)\n",
        "y = np.load(\"../app/model/y.npy\", allow_pickle=True)\n",
        "groups = np.load(\"../app/model/groups_job.npy\", allow_pickle=True)\n",
        "arts = joblib.load(\"../app/model/artifacts.joblib\",)\n",
        "print(\"feat_names:\", arts[\"feat_names\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "h_Mckn457kwu"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0.18971818,  1.        ,  1.        , ..., 31.        ,\n",
              "         1.        ,  0.        ],\n",
              "       [ 0.0813253 ,  1.        ,  1.        , ..., 10.        ,\n",
              "         1.        ,  0.125     ],\n",
              "       [ 0.19803253,  1.        ,  1.        , ..., 29.        ,\n",
              "         0.        ,  0.        ],\n",
              "       ...,\n",
              "       [ 0.13800335,  0.        ,  1.        , ..., 24.        ,\n",
              "         4.        ,  0.        ],\n",
              "       [ 0.29219967,  0.        ,  1.        , ..., 13.        ,\n",
              "         5.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  1.        , ...,  0.        ,\n",
              "         0.        ,  0.        ]])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtt0wakG7p2N"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ok] melhor modelo: rf\n",
            "[ok] AUC-PR (OOF): 0.1521\n",
            "[ok] F1_best (OOF): 0.2085 @thr=0.562\n",
            "[ok] P@3 (OOF): 0.1388 | P@5 (OOF): 0.1385\n",
            "[ok] artefatos salvos em C:\\Users\\kaio-\\mlops\\FIAP_PROJECTS_05\\build/\n"
          ]
        }
      ],
      "source": [
        "# train.py\n",
        "import json, os\n",
        "from pathlib import Path\n",
        "\n",
        "import joblib\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import average_precision_score, f1_score, precision_recall_curve\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "BUILD_DIR = Path(\"../app/model\")\n",
        "OUT_DIR = Path(\"../app/model\")  # pode separar se quiser\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def precision_at_k_per_job(y_true, y_score, job_ids, k=3):\n",
        "    \"\"\"\n",
        "    Calcula Precision@K por vaga e retorna a média.\n",
        "    Para cada job_id, ranqueia candidatos por score desc, pega top-K (ou todos se <K).\n",
        "    \"\"\"\n",
        "    assert len(y_true) == len(y_score) == len(job_ids)\n",
        "    jobs = np.unique(job_ids)\n",
        "    precs = []\n",
        "    for j in jobs:\n",
        "        mask = (job_ids == j)\n",
        "        if mask.sum() == 0:\n",
        "            continue\n",
        "        # ranking desc\n",
        "        idx = np.argsort(-y_score[mask])\n",
        "        topk = idx[:min(k, mask.sum())]\n",
        "        yk = y_true[mask][topk]\n",
        "        precs.append(yk.mean() if len(yk) > 0 else 0.0)\n",
        "    return float(np.mean(precs)) if len(precs) > 0 else 0.0\n",
        "\n",
        "def choose_best_threshold_by_f1(y_true, y_score):\n",
        "    \"\"\"\n",
        "    Encontra o threshold que maximiza F1 usando a curva de Precision-Recall.\n",
        "    \"\"\"\n",
        "    prec, rec, thr = precision_recall_curve(y_true, y_score)\n",
        "    # evita divisão por zero\n",
        "    denom = (prec + rec)\n",
        "    denom[denom == 0] = 1e-9\n",
        "    f1s = 2 * (prec * rec) / denom\n",
        "    # últimos pontos de PR podem não ter threshold; alinhar:\n",
        "    thr_full = np.r_[thr, [1.0]]  # garante mesmo tamanho que f1s\n",
        "    best_idx = int(np.nanargmax(f1s))\n",
        "    return float(thr_full[best_idx]), float(f1s[best_idx])\n",
        "\n",
        "def eval_cv(model_name, model, X, y, groups, k_s=[3,5], n_splits=5):\n",
        "    gkf = GroupKFold(n_splits=n_splits)\n",
        "    oof = np.zeros(len(y), dtype=float)\n",
        "\n",
        "    fold_metrics = []\n",
        "    for fold, (tr, va) in enumerate(gkf.split(X, y, groups)):\n",
        "        m = model\n",
        "        m.fit(X[tr], y[tr])\n",
        "        p = m.predict_proba(X[va])[:, 1]\n",
        "        oof[va] = p\n",
        "\n",
        "        ap = average_precision_score(y[va], p)\n",
        "        thr, f1_best = choose_best_threshold_by_f1(y[va], p)\n",
        "        metrics = {\"fold\": fold, \"auc_pr\": float(ap), \"f1_best\": float(f1_best), \"thr_best\": float(thr)}\n",
        "        # Precision@K por vaga (usando scores)\n",
        "        for K in k_s:\n",
        "            pk = precision_at_k_per_job(y[va], p, groups[va], k=K)\n",
        "            metrics[f\"p@{K}\"] = float(pk)\n",
        "        fold_metrics.append(metrics)\n",
        "\n",
        "    # agregados\n",
        "    ap_mean = float(average_precision_score(y, oof))\n",
        "    thr_global, f1_global = choose_best_threshold_by_f1(y, oof)\n",
        "\n",
        "    agg = {\n",
        "        \"model\": model_name,\n",
        "        \"auc_pr_oof\": ap_mean,\n",
        "        \"f1_best_oof\": float(f1_global),\n",
        "        \"thr_best_oof\": float(thr_global),\n",
        "    }\n",
        "    # p@K global (média por vaga no conjunto todo, usando OOF)\n",
        "    for K in k_s:\n",
        "        agg[f\"p@{K}_oof\"] = float(precision_at_k_per_job(y, oof, groups, k=K))\n",
        "\n",
        "    return oof, fold_metrics, agg\n",
        "\n",
        "def main():\n",
        "\n",
        "    X = np.load(BUILD_DIR / \"X.npy\", allow_pickle=True)\n",
        "    y = np.load(BUILD_DIR / \"y.npy\", allow_pickle=True)\n",
        "    groups = np.load(BUILD_DIR / \"groups_job.npy\", allow_pickle=True)\n",
        "    arts = joblib.load(BUILD_DIR / \"artifacts.joblib\")\n",
        "    feat_names = arts.get(\"feat_names\", [f\"f{i}\" for i in range(X.shape[1])])\n",
        "\n",
        "    # 2) Modelos candidatos\n",
        "    models = {\n",
        "        \"logreg\": LogisticRegression(max_iter=1000, class_weight=\"balanced\"),\n",
        "        \"rf\": RandomForestClassifier(\n",
        "            n_estimators=400,\n",
        "            max_depth=None,\n",
        "            min_samples_split=2,\n",
        "            min_samples_leaf=1,\n",
        "            n_jobs=-1,\n",
        "            class_weight=\"balanced_subsample\",\n",
        "            random_state=42,\n",
        "        ),\n",
        "         \"mlp\": MLPClassifier(\n",
        "                hidden_layer_sizes=(256,128),\n",
        "                activation=\"relu\",\n",
        "                alpha=1e-4,                # L2\n",
        "                learning_rate_init=1e-3,\n",
        "                max_iter=300,\n",
        "                early_stopping=True,\n",
        "                n_iter_no_change=15,\n",
        "                validation_fraction=0.1,\n",
        "                random_state=42\n",
        "            )\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    best_name, best_ap = None, -1.0\n",
        "    best_oof = None\n",
        "    for name, mdl in models.items():\n",
        "        oof, fold_metrics, agg = eval_cv(name, mdl, X, y, groups, k_s=[3,5], n_splits=5)\n",
        "        results[name] = {\"folds\": fold_metrics, \"agg\": agg}\n",
        "        if agg[\"auc_pr_oof\"] > best_ap:\n",
        "            best_ap = agg[\"auc_pr_oof\"]\n",
        "            best_name = name\n",
        "            best_oof = oof\n",
        "\n",
        "    # 3) Treina final no conjunto todo com o melhor modelo e salva\n",
        "    best_model = models[best_name]\n",
        "    best_model.fit(X, y)\n",
        "\n",
        "    # threshold ótimo a partir do OOF (guardamos para a API)\n",
        "    thr_best = results[best_name][\"agg\"][\"thr_best_oof\"]\n",
        "\n",
        "    joblib.dump(best_model, OUT_DIR / \"model.joblib\")\n",
        "    with open(OUT_DIR / \"report.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump({\n",
        "            \"best_model\": best_name,\n",
        "            \"results\": results,\n",
        "            \"feat_names\": feat_names,\n",
        "            \"threshold_best_f1\": thr_best,\n",
        "        }, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # salva OOF para auditoria/monitoramento (útil no Streamlit depois)\n",
        "    np.save(OUT_DIR / \"oof_scores.npy\", best_oof)\n",
        "    np.save(OUT_DIR / \"labels.npy\", y)\n",
        "    np.save(OUT_DIR / \"groups_job.npy\", groups)  # regrava pra conveniência\n",
        "\n",
        "    print(f\"[ok] melhor modelo: {best_name}\")\n",
        "    print(f\"[ok] AUC-PR (OOF): {results[best_name]['agg']['auc_pr_oof']:.4f}\")\n",
        "    print(f\"[ok] F1_best (OOF): {results[best_name]['agg']['f1_best_oof']:.4f} @thr={thr_best:.3f}\")\n",
        "    print(f\"[ok] P@3 (OOF): {results[best_name]['agg']['p@3_oof']:.4f} | P@5 (OOF): {results[best_name]['agg']['p@5_oof']:.4f}\")\n",
        "    print(f\"[ok] artefatos salvos em {OUT_DIR.resolve()}/\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lxXVZsTPONM"
      },
      "outputs": [],
      "source": [
        "df3 = pd.read_csv('/content/models/csv_df_final.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilhID2WjGvLf"
      },
      "outputs": [],
      "source": [
        "df3.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUC-78iYE1zz"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.11.8' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
            "\u001b[1;31mOr install 'ipykernel' using the command: 'c:/Users/kaio-/AppData/Local/Programs/Python/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# train_mlflow.py\n",
        "import os, json\n",
        "from pathlib import Path\n",
        "import joblib\n",
        "import numpy as np\n",
        "\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import average_precision_score, f1_score, precision_recall_curve\n",
        "\n",
        "# --------- Paths\n",
        "BUILD_DIR = Path(\"../app/model\")\n",
        "OUT_DIR = Path(\"../app/model\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ======================= utils métricas =======================\n",
        "def precision_at_k_per_job(y_true, y_score, job_ids, k=3):\n",
        "    assert len(y_true) == len(y_score) == len(job_ids)\n",
        "    jobs = np.unique(job_ids)\n",
        "    precs = []\n",
        "    for j in jobs:\n",
        "        mask = (job_ids == j)\n",
        "        if mask.sum() == 0:\n",
        "            continue\n",
        "        idx = np.argsort(-y_score[mask])\n",
        "        topk = idx[:min(k, mask.sum())]\n",
        "        yk = y_true[mask][topk]\n",
        "        precs.append(yk.mean() if len(yk) > 0 else 0.0)\n",
        "    return float(np.mean(precs)) if len(precs) > 0 else 0.0\n",
        "\n",
        "def choose_best_threshold_by_f1(y_true, y_score):\n",
        "    prec, rec, thr = precision_recall_curve(y_true, y_score)\n",
        "    denom = (prec + rec)\n",
        "    denom[denom == 0] = 1e-9\n",
        "    f1s = 2 * (prec * rec) / denom\n",
        "    thr_full = np.r_[thr, [1.0]]\n",
        "    best_idx = int(np.nanargmax(f1s))\n",
        "    return float(thr_full[best_idx]), float(f1s[best_idx])\n",
        "\n",
        "def eval_cv(model_name, model, X, y, groups, k_s=(3,5), n_splits=5, log_to_mlflow=True):\n",
        "    gkf = GroupKFold(n_splits=n_splits)\n",
        "    oof = np.zeros(len(y), dtype=float)\n",
        "    fold_metrics = []\n",
        "\n",
        "    for fold, (tr, va) in enumerate(gkf.split(X, y, groups)):\n",
        "        m = model\n",
        "        m.fit(X[tr], y[tr])\n",
        "        p = m.predict_proba(X[va])[:, 1]\n",
        "        oof[va] = p\n",
        "\n",
        "        ap = average_precision_score(y[va], p)\n",
        "        thr, f1_best = choose_best_threshold_by_f1(y[va], p)\n",
        "\n",
        "        metrics = {\"fold\": fold, \"auc_pr\": float(ap), \"f1_best\": float(f1_best), \"thr_best\": float(thr)}\n",
        "        for K in k_s:\n",
        "            pk = precision_at_k_per_job(y[va], p, groups[va], k=K)\n",
        "            metrics[f\"p@{K}\"] = float(pk)\n",
        "        fold_metrics.append(metrics)\n",
        "\n",
        "        # log por fold (como métricas separadas)\n",
        "        if log_to_mlflow:\n",
        "            mlflow.log_metrics({f\"{model_name}_fold{fold}_aucpr\": ap,\n",
        "                                f\"{model_name}_fold{fold}_f1best\": f1_best,\n",
        "                                f\"{model_name}_fold{fold}_thrbest\": thr,\n",
        "                                **{f\"{model_name}_fold{fold}_p@{K}\": metrics[f\"p@{K}\"] for K in k_s}\n",
        "                                }, step=fold)\n",
        "\n",
        "    # agregados (OOF)\n",
        "    ap_mean = float(average_precision_score(y, oof))\n",
        "    thr_global, f1_global = choose_best_threshold_by_f1(y, oof)\n",
        "    agg = {\n",
        "        \"model\": model_name,\n",
        "        \"auc_pr_oof\": ap_mean,\n",
        "        \"f1_best_oof\": float(f1_global),\n",
        "        \"thr_best_oof\": float(thr_global),\n",
        "    }\n",
        "    for K in k_s:\n",
        "        agg[f\"p@{K}_oof\"] = float(precision_at_k_per_job(y, oof, groups, k=K))\n",
        "\n",
        "    if log_to_mlflow:\n",
        "        mlflow.log_metrics({\n",
        "            f\"{model_name}_aucpr_oof\": ap_mean,\n",
        "            f\"{model_name}_f1best_oof\": f1_global,\n",
        "            f\"{model_name}_thrbest_oof\": thr_global,\n",
        "            **{f\"{model_name}_p@{K}_oof\": agg[f\"p@{K}_oof\"] for K in k_s}\n",
        "        })\n",
        "\n",
        "    return oof, fold_metrics, agg\n",
        "\n",
        "# ======================= treino com MLflow =======================\n",
        "def main():\n",
        "    # ---------- Config MLflow\n",
        "    tracking_uri = os.getenv(\"MLFLOW_TRACKING_URI\", \"file://\" + str((Path.cwd() / \"mlruns\").resolve()))\n",
        "    mlflow.set_tracking_uri(tracking_uri)\n",
        "    experiment_name = os.getenv(\"MLFLOW_EXPERIMENT_NAME\", \"datathon-recrutamento\")\n",
        "    mlflow.set_experiment(experiment_name)\n",
        "\n",
        "    # ---------- Dados\n",
        "    X = np.load(BUILD_DIR / \"X.npy\", allow_pickle=True)\n",
        "    y = np.load(BUILD_DIR / \"y.npy\", allow_pickle=True)\n",
        "    groups = np.load(BUILD_DIR / \"groups_job.npy\", allow_pickle=True)\n",
        "    arts = joblib.load(BUILD_DIR / \"artifacts.joblib\")\n",
        "    feat_names = arts.get(\"feat_names\", [f\"f{i}\" for i in range(X.shape[1])])\n",
        "\n",
        "    n_splits = int(os.getenv(\"CV_SPLITS\", \"5\"))\n",
        "\n",
        "    models = {\n",
        "        \"logreg\": LogisticRegression(max_iter=1000, class_weight=\"balanced\"),\n",
        "        \"rf\": RandomForestClassifier(\n",
        "            n_estimators=400,\n",
        "            max_depth=None,\n",
        "            min_samples_split=2,\n",
        "            min_samples_leaf=1,\n",
        "            n_jobs=-1,\n",
        "            class_weight=\"balanced_subsample\",\n",
        "            random_state=42,\n",
        "        ),\n",
        "        \"mlp\": MLPClassifier(\n",
        "            hidden_layer_sizes=(256,128),\n",
        "            activation=\"relu\",\n",
        "            alpha=1e-4,\n",
        "            learning_rate_init=1e-3,\n",
        "            max_iter=300,\n",
        "            early_stopping=True,\n",
        "            n_iter_no_change=15,\n",
        "            validation_fraction=0.1,\n",
        "            random_state=42\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    with mlflow.start_run(run_name=\"train_cv\") as run:\n",
        "        # tags/params gerais\n",
        "        mlflow.set_tags({\n",
        "            \"project\": \"vaga-match\",\n",
        "            \"stage\": os.getenv(\"STAGE\", \"dev\"),\n",
        "        })\n",
        "        mlflow.log_params({\n",
        "            \"n_features\": X.shape[1],\n",
        "            \"n_samples\": X.shape[0],\n",
        "            \"cv_splits\": n_splits\n",
        "        })\n",
        "        # hiperparâmetros dos modelos\n",
        "        for name, mdl in models.items():\n",
        "            # registra os get_params (prefixados pelo nome do modelo)\n",
        "            params = {f\"{name}__{k}\": v for k, v in mdl.get_params().items()}\n",
        "            # cuidado: mlflow só aceita tipos simples\n",
        "            clean_params = {k: (str(v) if not isinstance(v, (int, float, str, bool)) else v)\n",
        "                            for k, v in params.items()}\n",
        "            mlflow.log_params(clean_params)\n",
        "\n",
        "        # --------- CV e escolha do melhor\n",
        "        results = {}\n",
        "        best_name, best_ap = None, -1.0\n",
        "        best_oof = None\n",
        "\n",
        "        for name, mdl in models.items():\n",
        "            oof, fold_metrics, agg = eval_cv(name, mdl, X, y, groups, k_s=(3,5), n_splits=n_splits, log_to_mlflow=True)\n",
        "            results[name] = {\"folds\": fold_metrics, \"agg\": agg}\n",
        "            if agg[\"auc_pr_oof\"] > best_ap:\n",
        "                best_ap = agg[\"auc_pr_oof\"]\n",
        "                best_name = name\n",
        "                best_oof = oof\n",
        "\n",
        "        # --------- Treino final e salvamento\n",
        "        best_model = models[best_name]\n",
        "        best_model.fit(X, y)\n",
        "        thr_best = results[best_name][\"agg\"][\"thr_best_oof\"]\n",
        "\n",
        "        # salva local\n",
        "        joblib.dump(best_model, OUT_DIR / \"model.joblib\")\n",
        "        with open(OUT_DIR / \"report.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump({\n",
        "                \"best_model\": best_name,\n",
        "                \"results\": results,\n",
        "                \"feat_names\": feat_names,\n",
        "                \"threshold_best_f1\": thr_best,\n",
        "            }, f, ensure_ascii=False, indent=2)\n",
        "        np.save(OUT_DIR / \"oof_scores.npy\", best_oof)\n",
        "        np.save(OUT_DIR / \"labels.npy\", y)\n",
        "        np.save(OUT_DIR / \"groups_job.npy\", groups)\n",
        "\n",
        "        # --------- Log no MLflow\n",
        "        mlflow.log_metric(\"best_aucpr_oof\", best_ap)\n",
        "        mlflow.log_metric(\"best_thrbest_oof\", thr_best)\n",
        "        mlflow.log_param(\"best_model_name\", best_name)\n",
        "\n",
        "        # artefatos úteis\n",
        "        mlflow.log_artifact(OUT_DIR / \"report.json\", artifact_path=\"artifacts\")\n",
        "        mlflow.log_artifact(OUT_DIR / \"oof_scores.npy\", artifact_path=\"artifacts\")\n",
        "        mlflow.log_artifact(OUT_DIR / \"labels.npy\", artifact_path=\"artifacts\")\n",
        "        mlflow.log_artifact(OUT_DIR / \"groups_job.npy\", artifact_path=\"artifacts\")\n",
        "\n",
        "        # log do modelo no MLflow (com assinatura simples)\n",
        "        signature = None\n",
        "        try:\n",
        "            import mlflow.models.signature as msign\n",
        "            from mlflow.types.schema import Schema, ColSpec\n",
        "            signature = msign.ModelSignature(\n",
        "                inputs=Schema([ColSpec(\"double\", name) for name in feat_names]),\n",
        "                outputs=Schema([ColSpec(\"double\", \"score\")])\n",
        "            )\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # exemplo de entrada (apenas para rastreabilidade)\n",
        "        input_example = np.zeros((1, X.shape[1]))\n",
        "\n",
        "        mlflow.sklearn.log_model(\n",
        "            sk_model=best_model,\n",
        "            artifact_path=\"model\",\n",
        "            signature=signature,\n",
        "            input_example=input_example,\n",
        "            registered_model_name=os.getenv(\"MLFLOW_REGISTER_MODEL_NAME\") if os.getenv(\"MLFLOW_REGISTER_MODEL\", \"0\") == \"1\" else None\n",
        "        )\n",
        "\n",
        "        print(f\"[ok] melhor modelo: {best_name}\")\n",
        "        print(f\"[ok] AUC-PR (OOF): {results[best_name]['agg']['auc_pr_oof']:.4f}\")\n",
        "        print(f\"[ok] F1_best (OOF): {results[best_name]['agg']['f1_best_oof']:.4f} @thr={thr_best:.3f}\")\n",
        "        print(f\"[ok] P@3 (OOF): {results[best_name]['agg']['p@3_oof']:.4f} | P@5 (OOF): {results[best_name]['agg']['p@5_oof']:.4f}\")\n",
        "        print(f\"[ok] artefatos salvos em {OUT_DIR.resolve()}/\")\n",
        "        print(f\"[ok] run_id: {run.info.run_id}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
